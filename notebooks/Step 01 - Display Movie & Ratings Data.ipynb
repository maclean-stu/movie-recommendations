{
    "nbformat_minor": 0, 
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0"
        }
    }, 
    "cells": [
        {
            "source": "## Display Movie data that has been loaded into the Coudant noSQL database\n\nExample uses Spark SQL with a Cloudant data source\n\nThis sample notebook is written in Python and expects the Python 2.7.5 runtime. Make sure the kernel is started and you are connect to it when executing this notebook.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "# Import Python stuff\nimport pprint\nfrom collections import Counter", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": null, 
            "source": "# Import PySpark stuff\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import udf, asc, desc\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql.types import IntegerType", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### 1. Work with the Spark Context\nA Spark Context handle sc is available with every notebook create in the Spark Service.  \nUse it to understand the Spark version used, the environment settings, and create a Spark SQL Context object off of it.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "sc.version", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": null, 
            "source": "# sc is an existing SparkContext.\nsqlContext = SQLContext(sc)", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### 2. Work with a Cloudant database\nA Dataframe object can be created directly from a Cloudant database. To configure the database as source, pass these options:  \n1 - package name that provides the classes (like CloudantDataSource) implemented in the connector to extend BaseRelation. For the Cloudant Spark connector this will be com.cloudant.spark  \n2 - cloudant.host parameter to pass the Cloudant account name  \n3 - cloudant.user parameter to pass the Cloudant user name  \n4 - cloudant.password parameter to pass the Cloudant account password  \n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": null, 
            "source": "cloudantdata = sqlContext.read.format(\"com.cloudant.spark\").\\\noption(\"cloudant.host\",credentials['host']).\\\noption(\"cloudant.username\",credentials['username']).\\\noption(\"cloudant.password\",credentials['password']).\\\nload(\"moviedb\")", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### 3. Work with a Dataframe\nAt this point all transformations and functions should behave as specified with Spark SQL. (http://spark.apache.org/sql/)  \n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "cloudantdata.printSchema()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": null, 
            "source": "cloudantdata.count()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": null, 
            "source": "cloudantdata.select(\"name\", \"url\").show()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": null, 
            "source": "import pandas as pd\npandaDf = cloudantdata.select(\"name\").toPandas()\nprint(pandaDf, 10)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "## Display the \"randomly generated\" Ratings from Object Storage\n\nDrag and drop the ratings file onto the files rectangle. then use the DSX code insertion feature to create a hadoop configuration to allow Spark to access the object storage the ratings file resides on.", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "Read the file from object storage and display the first 10 records.  \n\nNote: change the code insertion 'path_n' variable above to 'ratingsfile'", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "data_rdd = sc.textFile(ratingsfile)\ndata_rdd.take(10)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": null, 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat": 4
}