{"metadata": {"language_info": {"file_extension": ".py", "version": "2.7.11", "mimetype": "text/x-python", "pygments_lexer": "ipython2", "name": "python", "codemirror_mode": {"version": 2, "name": "ipython"}, "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 2 with Spark 2.0", "language": "python", "name": "python2-spark20"}}, "nbformat": 4, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "## Display Movies from the Coudant database\n\nExample uses Spark SQL with a Cloudant data source\n\nThis sample notebook is written in Python and expects the Python 2.7.5 runtime. Make sure the kernel is started and you are connect to it when executing this notebook."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# Import Python stuff\nimport pprint\nfrom collections import Counter"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# Import PySpark stuff\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import udf, asc, desc\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql.types import IntegerType"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1. Work with the Spark Context\nA Spark Context handle sc is available with every notebook create in the Spark Service.  \nUse it to understand the Spark version used, the environment settings, and create a Spark SQL Context object off of it."}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": null, "source": "sc.version"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# sc is an existing SparkContext.\nsqlContext = SQLContext(sc)"}, {"metadata": {}, "cell_type": "markdown", "source": "### 2. Work with a Cloudant database\nA Dataframe object can be created directly from a Cloudant database. To configure the database as source, pass these options:  \n1 - package name that provides the classes (like CloudantDataSource) implemented in the connector to extend BaseRelation. For the Cloudant Spark connector this will be com.cloudant.spark  \n2 - cloudant.host parameter to pass the Cloudant account name  \n3 - cloudant.user parameter to pass the Cloudant user name  \n4 - cloudant.password parameter to pass the Cloudant account password  \n"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# @hidden_cell\ncredentials = {\n  'username':'f16bd2e3-8b90-4581-97d1-43dcd23c3ccd-bluemix',\n  'password':\"\"\"28423678f7490bca3312b2f4cd4d839a9e7e587613cddedf8eaaae959a8d2d28\"\"\",\n  'host':'f16bd2e3-8b90-4581-97d1-43dcd23c3ccd-bluemix.cloudant.com',\n  'port':'443',\n  'url':'https://f16bd2e3-8b90-4581-97d1-43dcd23c3ccd-bluemix:28423678f7490bca3312b2f4cd4d839a9e7e587613cddedf8eaaae959a8d2d28@f16bd2e3-8b90-4581-97d1-43dcd23c3ccd-bluemix.cloudant.com'\n}\n"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": null, "source": "cloudantdata = sqlContext.read.format(\"com.cloudant.spark\").\\\noption(\"cloudant.host\",credentials['host']).\\\noption(\"cloudant.username\",credentials['username']).\\\noption(\"cloudant.password\",credentials['password']).\\\nload(\"moviedb\")"}, {"metadata": {}, "cell_type": "markdown", "source": "### 3. Work with a Dataframe\nAt this point all transformations and functions should behave as specified with Spark SQL. (http://spark.apache.org/sql/)  \n"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": null, "source": "cloudantdata.printSchema()"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": null, "source": "cloudantdata.count()"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": null, "source": "cloudantdata.select(\"name\", \"url\").show()"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": null, "source": "import pandas as pd\npandaDf = cloudantdata.select(\"name\").toPandas()\nprint(pandaDf, 10)"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Display the \"randomly generated\" Ratings from Object Storage\n\nDrag and drop the ratings file onto the files rectangle. then use the DSX code insertion feature to create a hadoop configuration to allow Spark to access the object storage the ratings file resides on."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "\nfrom pyspark.sql import SparkSession\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_5a2eb3130fdf4523ad2d610fb1f3d6f9(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', 'a47485e45a62467a92be88568bb933cd')\n    hconf.set(prefix + '.username', '14b0c25dc91d4f3cb54a5b1bcb714eb7')\n    hconf.set(prefix + '.password', 'kwBOG#^Z28Z{vs)y')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_5a2eb3130fdf4523ad2d610fb1f3d6f9(name)\n\nspark = SparkSession.builder.getOrCreate()\n\n# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n# The SparkSession object is already initalized for you.\n# The following variable contains the path to your file on your Object Storage.\nratingsfile = \"swift://MovieRecommender.\" + name + \"/ratings.dat\"\n"}, {"metadata": {}, "cell_type": "markdown", "source": "Read the file from object storage and display the first 10 records.  \n\nNote: change the code insertion 'path_n' variable above to 'ratingsfile'"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": null, "source": "data_rdd = sc.textFile(ratingsfile)\ndata_rdd.take(10)"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": ""}], "nbformat_minor": 0}